---
# MobileFaceNet_112 for face recognition. Compatible with MAX78002.

arch: ai87netmobilefacenet_112
dataset: vggface2_faceid

layers:
  # Layer 0: pre_stage. in 3ch, out 64 ch
  - processors: 0x0000000000000007
    in_offset: 0x0000
    out_offset: 0x8000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
    streaming: true
  # Layer 1: Dwise. in 64ch, out 64ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 64
    activate: ReLU
    max_pool: 2
    pool_stride: 2
    streaming: true
  # Layer 2: Bottleneck-0, n=0, conv1. in 64ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 3: Bottleneck-1, n=0, conv2. in 128ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 128
    activate: ReLU
    max_pool: 2
    pool_stride: 2
  # Layer 4: Bottleneck-1, n=0, conv3. in 128 ch, out 64 ch.
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: None
  # Layer 5: Bottleneck-1, n=1, conv1. in 64ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 6: Bottleneck-1, n=1, conv2. in 128 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 128
    activate: ReLU
  # Layer 7: Bottleneck-1, n=1, conv3. in 128 ch, out 64 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activate: None
  # Layer 8: Bottleneck-1, n=1, Reform input layer
  - in_offset: 0x0000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [4]
  # Layer 9: Bottleneck-1, n=1, Residual add
  - in_offset: 0xa000
    out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [7, 8]
  # Layer 10: Bottleneck-1, n=2, conv1. in 64ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 11: Bottleneck-1, n=2, conv2. in 128 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 128
    activate: ReLU
  # Layer 12: Bottleneck-1, n=2, conv3. in 128 ch, out 64 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activate: None
  # Layer 13: Bottleneck-1, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [9]
  # Layer 14: Bottleneck-1, n=2, Residual add
  - in_offset: 0xa000
    out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [12, 13]
  # Layer 15: Bottleneck-1, n=3, conv1. in 64ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 16: Bottleneck-1, n=3, conv2. in 128 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 128
    activate: ReLU
  # Layer 17: Bottleneck-1, n=3, conv3. in 128 ch, out 64 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 18: Bottleneck-1, n=3, Reform input layer
  - in_offset: 0x0000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [14]
  # Layer 19: Bottleneck-1, n=3, Residual add
  - in_offset: 0xa000
    out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [17, 18]
  # Layer 20: Bottleneck-1, n=4, conv1. in 64ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 21: Bottleneck-1, n=4, conv2. in 128 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 128
    activate: ReLU
  # Layer 22: Bottleneck-1, n=4, conv3. in 128 ch, out 64 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 23: Bottleneck-1, n=4, Reform input layer
  - in_offset: 0x0000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [19]
  # Layer 24: Bottleneck-1, n=4, Residual add
  - in_offset: 0xa000
    out_offset: 0x6000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [22, 23]
  # Layer 25: Bottleneck-2, n=0, conv1. in 64ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 26: Bottleneck-2, n=0, conv2. in 256ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
    max_pool: 2
    pool_stride: 2
  # Layer 27: Bottleneck-2, n=0, conv3. in 256 ch, out 128 ch.
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: None
  # Layer 28: Bottleneck-3, n=0, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 29: Bottleneck-3, n=0, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 30: Bottleneck-3, n=0, conv3. in 256 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 31: Bottleneck-3, n=0, Reform input layer
  - in_offset: 0x0000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [27]
  # Layer 32: Bottleneck-3, n=0, Residual add
  - in_offset: 0xa000
    out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [30, 31]
  # Layer 33: Bottleneck-3, n=1, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 34: Bottleneck-3, n=1, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 35: Bottleneck-3, n=1, conv3. in 256 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 36: Bottleneck-3, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [32]
  # Layer 37: Bottleneck-3, n=1, Residual add
  - in_offset: 0xa000
    out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [35, 36]
  # Layer 38: Bottleneck-3, n=2, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 39: Bottleneck-3, n=2, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 40: Bottleneck-3, n=2, conv3. in 256 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 41: Bottleneck-3, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [37]
  # Layer 42: Bottleneck-3, n=2, Residual add
  - in_offset: 0xa000
    out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [40, 41]
  # Layer 43: Bottleneck-3, n=3, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 44: Bottleneck-3, n=3, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 45: Bottleneck-3, n=3, conv3. in 256 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 46: Bottleneck-3, n=3, Reform input layer
  - in_offset: 0x0000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [42]
  # Layer 47: Bottleneck-3, n=3, Residual add
  - in_offset: 0xa000
    out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [45, 46]
  # Layer 48: Bottleneck-3, n=4, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 49: Bottleneck-3, n=4, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 50: Bottleneck-3, n=4, conv3. in 256 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 51: Bottleneck-3, n=4, Reform input layer
  - in_offset: 0x4000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [47]
  # Layer 52: Bottleneck-1, n=4, Residual add
  - in_offset: 0xa000
    out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [50, 51]
  # Layer 53: Bottleneck-3, n=5, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 54: Bottleneck-3, n=5, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 55: Bottleneck-3, n=5, conv3. in 256 ch, out 128ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 56: Bottleneck-3, n=5, Reform input layer
  - in_offset: 0x0000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [52]
  # Layer 57: Bottleneck-3, n=5, Residual add
  - in_offset: 0xa000
    out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [55, 56]
  # Layer 58: Bottleneck-4, n=0, conv1. in 128ch, out 512 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 59: Bottleneck-4, n=0, conv2. in 512ch, out 512 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 512
    activate: ReLU
    max_pool: 2
    pool_stride: 2
  # Layer 60: Bottleneck-4, n=0, conv3. in 512 ch, out 128 ch.
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: None
  # Layer 61: Bottleneck-5, n=0, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 62: Bottleneck-5, n=0, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x8000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 63: Bottleneck-5, n=0, conv3. in 256 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 64: Bottleneck-5, n=1, Reform input layer
  - in_offset: 0x0000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [60]
  # Layer 65: Bottleneck-5, n=0, Residual add
  - in_offset: 0xa000
    out_offset: 0x4000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [63, 64]
  # Layer 66: Bottleneck-5, n=1, conv1. in 128ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 67: Bottleneck-5, n=1, conv2. in 256 ch, out 256 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 256
    activate: ReLU
  # Layer 68: Bottleneck-5, n=1, conv3. in 256 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    write_gap: 1
    pad: 0
    activate: None
  # Layer 69: Bottleneck-5, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0xa004
    processors: 0xffffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [65]
  # Layer 70: Bottleneck-5, n=1, Residual add
  - in_offset: 0xa000
    out_offset: 0x0000
    processors: 0xffffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [68, 69]
  # Layer 71: post-stage in 128 ch, out 128 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 72: output layer in 128 ch, out 64 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0xa000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: None
    avg_pool: [7, 7]
    pool_stride: 1
