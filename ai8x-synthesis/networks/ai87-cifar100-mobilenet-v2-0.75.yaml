---
# MobileNet-v2 model with scale=0.75 for cifar-100. Compatible with MAX78002.

arch: ai87netmobilenetv2cifar100_m0_75
dataset: cifar100

layers:
  # Layer 0: pre_stage. in 3ch, out 24 ch
  - processors: 0x0000000000000007
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    activate: ReLU
  # Bottleneck-0, n=0, conv1 empty as expansion factor = 1.
  # Layer 1: Bottleneck-0, n=0, conv2. in 24ch, out 24 ch
  - processors: 0x0000000000ffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 24
    activate: ReLU
  # Layer 2: Bottleneck-0, n=0, conv3. in 24ch, out 12 ch
  - processors: 0x0000000000ffffff
    output_processors: 0x0000000000000fff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
  # Layer 3: Bottleneck-1, n=0, conv1. in 12ch, out 72 ch
  - processors: 0x0000000000000fff
    output_processors: 0x0000fffffffff000  # 0xfffffffff0000000
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 4: Bottleneck-1, n=0, conv2. in 72 ch, out 72 ch.
  - processors: 0x0000fffffffff000  # 0xfffffffff0000000
    output_processors: 0x0000fffffffff000  # 0xfffffffff0000000
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 72
    activate: ReLU
    max_pool: 2
    pool_stride: 2
  # Layer 5: Bottleneck-1, n=0, conv3. in 72ch, out 20 ch
  - processors: 0x0000fffffffff000
    output_processors: 0x00000000000fffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
  # Layer 6: Bottleneck-1, n=1, conv1. in 20 ch, out 120 ch
  - processors: 0x00000000000fffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 7: Bottleneck-1, n=1, conv2. in 120 ch, out 120 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 120
    activate: ReLU
  # Layer 8: Bottleneck-1, n=1, conv3
  - processors: 0x0fffffffffffffff
    output_processors: 0x00000000000fffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 9: Bottleneck-1, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x00000000000fffff
    operation: passthrough
    write_gap: 1
    in_sequences: [5]
  # Layer 10: Bottleneck-1, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x00000000000fffff
    operation: none
    eltwise: add
    in_sequences: [8, 9]
  # Layer 11: Bottleneck-2, n=0, conv1. in 20ch, out 120 ch
  - processors: 0x00000000000fffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 12: Bottleneck-2, n=0, conv2. in 120 ch, out 120 ch.
  - processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 120
    activate: ReLU
    max_pool: 2
    pool_stride: 2
  # Layer 13: Bottleneck-2, n=0, conv3. in 120ch, out 24 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000000000ffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
  # Layer 14: Bottleneck-2, n=1, conv1. in 24 ch, out 144 ch
  - processors: 0x0000000000ffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 15: Bottleneck-2, n=1, conv2. in 144 ch, out 144 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 144
    activate: ReLU
  # Layer 16: Bottleneck-2, n=1, conv3, in 144 ch, out ch 24
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000000000ffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 17: Bottleneck-2, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000000ffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [13]
  # Layer 18: Bottleneck-2, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000000ffffff
    operation: none
    eltwise: add
    in_sequences: [16, 17]
  # Layer 19: Bottleneck-2, n=2, conv1. in 24 ch, out 144 ch
  - processors: 0x0000000000ffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 20: Bottleneck-2, n=2, conv2. in 144 ch, out 144 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 144
    activate: ReLU
  # Layer 21: Bottleneck-2, n=2, conv3, in 144 ch, out ch 24
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000000000ffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 22: Bottleneck-2, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000000ffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [18]
  # Layer 23: Bottleneck-2, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000000ffffff
    operation: none
    eltwise: add
    in_sequences: [21, 22]
  # Layer 24: Bottleneck-3, n=0, conv1. in 24ch, out 144 ch
  - processors: 0x0000000000ffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 25: Bottleneck-3, n=0, conv2. in 144 ch, out 144 ch. -> 4x4
  - processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 144
    activate: ReLU
    max_pool: 2
    pool_stride: 2
  # Layer 26: Bottleneck-3, n=0, conv3. in 144 ch, out 48ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
  # Layer 27: Bottleneck-3, n=1, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 28: Bottleneck-3, n=1, conv2. in 288 ch, out 288 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 288
    activate: ReLU
  # Layer 29: Bottleneck-3, n=1, conv3, in 288 ch, out ch 48
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 30: Bottleneck-3, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [26]
  # Layer 31: Bottleneck-3, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add
    in_sequences: [29, 30]
  # Layer 32: Bottleneck-3, n=2, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 33: Bottleneck-3, n=2, conv2. in 288 ch, out 288 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 288
    activate: ReLU
  # Layer 34: Bottleneck-3, n=2, conv3, in 288 ch, out ch 48
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 35: Bottleneck-3, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [31]
  # Layer 36: Bottleneck-3, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add
    in_sequences: [34, 35]
  # Layer 37: Bottleneck-3, n=3, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 38: Bottleneck-3, n=3, conv2. in 288 ch, out 288 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 288
    activate: ReLU
  # Layer 39: Bottleneck-3, n=3, conv3, in 288 ch, out ch 48
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000ffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 40: Bottleneck-3, n=3, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000ffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [36]
  # Layer 41: Bottleneck-3, n=3, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000ffffffffffff
    operation: none
    eltwise: add
    in_sequences: [39, 40]
  # Layer 42: Bottleneck-4, n=0, conv1. in 48 ch, out 288 ch
  - processors: 0x0000ffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 43: Bottleneck-4, n=0, conv2. in 288 ch, out 288 ch.
  - processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 288
    activate: ReLU
  # Layer 44: Bottleneck-4, n=0, conv3. in 288, out 72 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0000000fffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
  # Layer 45: Bottleneck-4, n=1, conv1. in 72 ch, out 432 ch
  - processors: 0x0000000fffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 46: Bottleneck-4, n=1, conv2. in 432 ch, out 432 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 432
    activate: ReLU
  # Layer 47: Bottleneck-4, n=1, conv3, in 432 ch, out ch 72
  - processors: 0xffffffffffffffff
    output_processors: 0x0000000fffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 48: Bottleneck-4, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000fffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [44]
  # Layer 49: Bottleneck-4, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000fffffffff
    operation: none
    eltwise: add
    in_sequences: [47, 48]
  # Layer 50: Bottleneck-4, n=2, conv1. in 72 ch, out 432 ch
  - processors: 0x0000000fffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 51: Bottleneck-4, n=2, conv2. in 432 ch, out 432 ch
  - processors: 0xffffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 432
    activate: ReLU
  # Layer 52: Bottleneck-4, n=2, conv3, in 432 ch, out ch 72
  - processors: 0xffffffffffffffff
    output_processors: 0x0000000fffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 53: Bottleneck-4, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0000000fffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [49]
  # Layer 54: Bottleneck-4, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0000000fffffffff
    operation: none
    eltwise: add
    in_sequences: [52, 53]
  # Layer 55: Bottleneck-5, n=0, conv1. in 72 ch, out 432 ch
  - processors: 0x0000000fffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 56: Bottleneck-5, n=0, conv2. in 432 ch, out 432 ch.
  - processors: 0xffffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 432
    activate: ReLU
  # Layer 57: Bottleneck-5, n=0, conv3. in 432 ch, out 120 ch
  - processors: 0xffffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
  # Layer 58: Bottleneck-5, n=1, conv1. in 120 ch, out 720 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 59: Bottleneck-5, n=1, conv2. in 720 ch, out 720 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 720
    activate: ReLU
  # Layer 60: Bottleneck-5, n=1, conv3, in 720 ch, out ch 120
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 61: Bottleneck-5, n=1, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0fffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [57]
  # Layer 62: Bottleneck-5, n=1, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0fffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [60, 61]
  # Layer 63: Bottleneck-5, n=2, conv1. in 120 ch, out 720 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 64: Bottleneck-5, n=2, conv2. in 720 ch, out 720 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 720
    activate: ReLU
  # Layer 65: Bottleneck-5, n=2, conv3, in 720 ch, out ch 120
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    write_gap: 1
    activation: None
  # Layer 66: Bottleneck-5, n=2, Reform input layer
  - in_offset: 0x4000
    out_offset: 0x0004
    processors: 0x0fffffffffffffff
    operation: passthrough
    write_gap: 1
    in_sequences: [62]
  # Layer 67: Bottleneck-5, n=2, Residual add
  - in_offset: 0x0000
    out_offset: 0x4000
    processors: 0x0fffffffffffffff
    operation: none
    eltwise: add
    in_sequences: [65, 66]
  # Layer 68: Bottleneck-6, n=0, conv1. in 120 ch, out 720 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 69: Bottleneck-6, n=0, conv2. in 720 ch, out 720 ch.
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 3x3
    pad: 1
    groups: 720
    activate: ReLU
  # Layer 70: Bottleneck-6, n=0, conv3. in 720 ch, out 240 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0x0fffffffffffffff
    out_offset: 0x4000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activation: None
  # Layer 71: post-stage in 240 ch, out 960 ch
  - processors: 0x0fffffffffffffff
    output_processors: 0xffffffffffffffff
    out_offset: 0x2000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    activate: ReLU
  # Layer 72: classifier in 960 ch, out 100 ch -> 1x1
  - processors: 0xffffffffffffffff
    output_processors: 0x000fffffffffffff
    out_offset: 0x0000
    operation: Conv2d
    kernel_size: 1x1
    pad: 0
    avg_pool: 4
    pool_stride: 4
    output_width: 32
    activate: None
